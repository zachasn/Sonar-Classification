{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e8c94c2f-ab44-4318-941a-2d474e1e8441",
      "metadata": {
        "id": "e8c94c2f-ab44-4318-941a-2d474e1e8441"
      },
      "source": [
        "---\n",
        "<h1 style=\"text-align: center;\">\n",
        "CSCI 4521: Applied Machine Learning (Fall 2024)\n",
        "</h1>\n",
        "\n",
        "<h1 style=\"text-align: center;\">\n",
        "Homework 5\n",
        "</h1>\n",
        "\n",
        "<h3 style=\"text-align: center;\">\n",
        "(Due Tue, Nov. 26, 11:59 PM CT)\n",
        "</h3>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bafea59-8cdb-4666-8016-d3d2341fd062",
      "metadata": {
        "id": "4bafea59-8cdb-4666-8016-d3d2341fd062"
      },
      "source": [
        "![nn.png](attachment:fde9d58f-62e0-4c07-aacb-8334e3ef1027.png)\n",
        "\n",
        "Image from https://aibusiness.com/ml/how-neural-networks-can-think-like-humans-and-why-it-matters#close-modal"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf573c05-3b91-4fe5-a0f4-9038560c93d5",
      "metadata": {
        "id": "bf573c05-3b91-4fe5-a0f4-9038560c93d5"
      },
      "source": [
        "### In this homework, your task is to experiment with fully-connected, feed-forward neural networks to predict whether a sonar signal bounces off a metal cylinder or a cylindrical rock. The only data you have available is the sonar data in the dataset `sonar_csci4521_hw5.csv`. Each row is a sample and columns are the sonar features, and the last column is the label of metal (\"M\") or rock (\"R\").\n",
        "\n",
        "### You do not need to clean or preprocess the data in this homework except encoding the label using the `LabelEncoder`; focus on building and training neural networks. You still need to determine what kind of neural network to use, which and how to tune any hyperparameters, how to measure performance, which models to select, and which final model to use. We do expect that you will try a few different architectures (e.g., number of layers, number of units in each layer), activation functions, and gradient descent algorithms (e.g., stochastic gradient descent, Adagrad, RMSprop, Adam). We also expect that you will tune hyperparameters (not necessarily with cross validation but definitely only using the training dataset) and measure the performance of the final model on a held-out test set. Additionally, we expect you to track the performance of your experiments using Tensorboard, for example, track the average loss and accuracy per epoch on the training and test sets."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4753a60e-803f-40a9-b64d-6e34d98ee1bf",
      "metadata": {
        "id": "4753a60e-803f-40a9-b64d-6e34d98ee1bf"
      },
      "source": [
        "### You must use **PyTorch** to build and train your neural network, no other packages will be accepted (for example, you cannot use Tensorflow). If you use anything other than PyTorch to build your network, you will receive no credit for this homework. Make sure to write and submit clean, working code. Reminder, you cannot use ChatGPT or similar technologies. Please see the syllabus for more details."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "251f9f7c-f61d-41d9-a0bc-03803ee34377",
      "metadata": {
        "id": "251f9f7c-f61d-41d9-a0bc-03803ee34377"
      },
      "source": [
        "### You also need to submit a short report of your work describing all steps you took, explanations of why you took those steps, results, what you learned, how you might use what you learned in the future, and your conclusions. We expect the report to be well-written and clearly describe everything you've done and why."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d379108-9103-43da-ac09-2074efe2737c",
      "metadata": {
        "id": "7d379108-9103-43da-ac09-2074efe2737c"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd51e918-fc0c-40df-9e41-ea75d18637ea",
      "metadata": {
        "id": "dd51e918-fc0c-40df-9e41-ea75d18637ea"
      },
      "source": [
        "### Write your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "57d1f6e4-02c7-4ee5-8595-5ea438c46fc2",
      "metadata": {
        "id": "57d1f6e4-02c7-4ee5-8595-5ea438c46fc2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "c7c61462-2afd-4e05-8f3a-786ff1aca712",
      "metadata": {
        "id": "c7c61462-2afd-4e05-8f3a-786ff1aca712"
      },
      "outputs": [],
      "source": [
        "def load_data(path: str) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "  '''\n",
        "    Load data from the specified path.\n",
        "    path: str - file path\n",
        "    return: X, y - the features and labels\n",
        "  '''\n",
        "  data = pd.read_csv(path)\n",
        "  X = data.iloc[:, :-1].values\n",
        "  y = data.iloc[:, -1].values\n",
        "  y = LabelEncoder().fit_transform(y)\n",
        "  X = torch.tensor(X, dtype=torch.float32)\n",
        "  y = torch.tensor(y, dtype=torch.long)\n",
        "  return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "38090cdb-f597-4f21-83c8-4132396dfb88",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38090cdb-f597-4f21-83c8-4132396dfb88",
        "outputId": "ee073067-11ed-4a7d-a244-3ae8395f3d6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([207, 60]) torch.Size([207])\n"
          ]
        }
      ],
      "source": [
        "X, y = load_data(\"sonar_csci4521_hw5.csv\")\n",
        "print(X.shape, y.shape)\n",
        "writer = SummaryWriter(log_dir='logs')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "e2461aa2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2461aa2",
        "outputId": "5b243d12-998b-4ea6-ed0a-9df4784e5883"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train: torch.Size([144, 60]), y_train: torch.Size([144])\n",
            "x_val: torch.Size([21, 60]), y_val: torch.Size([21])\n",
            "x_test: torch.Size([42, 60]), y_test: torch.Size([42])\n"
          ]
        }
      ],
      "source": [
        "# split data using 70/20/10 split ratio\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.125, random_state=1)\n",
        "\n",
        "print(f\"x_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
        "print(f\"x_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
        "print(f\"x_test: {X_test.shape}, y_test: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "821c4a42",
      "metadata": {
        "id": "821c4a42"
      },
      "outputs": [],
      "source": [
        "def build_nn(architecture: int, bias: bool=True):\n",
        "  '''\n",
        "    Build a neural network model with the specified architecture\n",
        "    architecture: int - the architecture of the model\n",
        "    bias: bool - whether to include bias or not\n",
        "    return: nn.Sequential - the model\n",
        "  '''\n",
        "\n",
        "  if architecture == 1: # 3 layers with ReLU activation function\n",
        "   return nn.Sequential(\n",
        "      nn.Linear(60, 30, bias=bias),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(30, 15, bias=bias),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(15, 2, bias=bias)\n",
        "   )\n",
        "  elif architecture == 2: # 5 layers with Tanh activation function\n",
        "    return nn.Sequential(\n",
        "      nn.Linear(60, 30, bias=bias),\n",
        "      nn.Tanh(),\n",
        "      nn.Linear(30, 15, bias=bias),\n",
        "      nn.Tanh(),\n",
        "      nn.Linear(15, 7, bias=bias),\n",
        "      nn.Tanh(),\n",
        "      nn.Linear(7, 3, bias=bias),\n",
        "      nn.Tanh(),\n",
        "      nn.Linear(3, 2, bias=bias)\n",
        "    )\n",
        "  elif architecture == 3: # 7 layers with Sigmoid activation function\n",
        "    return nn.Sequential(\n",
        "      nn.Linear(60, 50, bias=bias),\n",
        "      nn.Sigmoid(),\n",
        "      nn.Dropout(),\n",
        "      nn.Linear(50, 40, bias=bias),\n",
        "      nn.Sigmoid(),\n",
        "      nn.Dropout(),\n",
        "      nn.Linear(40, 30, bias=bias),\n",
        "      nn.Sigmoid(),\n",
        "      nn.Dropout(),\n",
        "      nn.Linear(30, 20, bias=bias),\n",
        "      nn.Sigmoid(),\n",
        "      nn.Dropout(),\n",
        "      nn.Linear(20, 10, bias=bias),\n",
        "      nn.Sigmoid(),\n",
        "      nn.Dropout(),\n",
        "      nn.Linear(10, 5, bias=bias),\n",
        "      nn.Sigmoid(),\n",
        "      nn.Dropout(),\n",
        "      nn.Linear(5, 2, bias=bias)\n",
        "    )\n",
        "  else: # 6 layers with Tanh activation function\n",
        "    return nn.Sequential(\n",
        "      nn.Linear(60, 50, bias=bias),\n",
        "      nn.Tanh(),\n",
        "      nn.Linear(50, 40, bias=bias),\n",
        "      nn.Tanh(),\n",
        "      nn.Linear(40, 20, bias=bias),\n",
        "      nn.Tanh(),\n",
        "      nn.Linear(20, 15, bias=bias),\n",
        "      nn.Tanh(),\n",
        "      nn.Linear(15, 10, bias=bias),\n",
        "      nn.Tanh(),\n",
        "      nn.Linear(10, 2, bias=bias)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "0b6de1d1",
      "metadata": {
        "id": "0b6de1d1"
      },
      "outputs": [],
      "source": [
        "def train_loop(model, data_loader, loss_fn, optimizer):\n",
        "  '''\n",
        "    code from lecture 22\n",
        "    Train the model using the training data.\n",
        "    model: nn.Module - the model to train\n",
        "    data_loader: DataLoader - the data loader for training data\n",
        "    loss_fn: nn.CrossEntropyLoss - the loss function\n",
        "    optimizer: optim - the optimizer\n",
        "    return: float - the loss of the model\n",
        "  '''\n",
        "  model.train()\n",
        "  epoch_loss = 0\n",
        "\n",
        "  for batch, (X, y) in enumerate(data_loader):\n",
        "    pred = model(X)\n",
        "    loss = loss_fn(pred, y)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    epoch_loss += loss.item()\n",
        "  return epoch_loss\n",
        "\n",
        "def test_loop(model, data_loader, loss_fn):\n",
        "  '''\n",
        "    code from lecture 22\n",
        "    Test the model using the validation data.\n",
        "    model: nn.Module - the model to test\n",
        "    data_loader: DataLoader - the data loader for validation data\n",
        "    loss_fn: nn.CrossEntropyLoss - the loss function\n",
        "    return: float - the loss of the model\n",
        "  '''\n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X, y in data_loader:\n",
        "      pred = model(X)\n",
        "      loss = loss_fn(pred, y)\n",
        "\n",
        "      test_loss += loss.item()\n",
        "  return test_loss\n",
        "\n",
        "\n",
        "def calculate_accuracy(model, data_loader):\n",
        "  '''\n",
        "    Calculate the accuracy of the model.\n",
        "    model: nn.Module - the model to test\n",
        "    data_loader: DataLoader - the data loader for the data\n",
        "    return: float - the accuracy score of the model\n",
        "  '''\n",
        "  model.eval()\n",
        "  true_y = []\n",
        "  pred_y = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X, y in data_loader:\n",
        "      pred = model(X)\n",
        "      _, predicted = torch.max(pred, 1)\n",
        "      true_y.extend(y.tolist())\n",
        "      pred_y.extend(predicted.tolist())\n",
        "  return accuracy_score(true_y, pred_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "b957d2bf",
      "metadata": {
        "id": "b957d2bf"
      },
      "outputs": [],
      "source": [
        "def parameter_tuning(architecture: int, optimizers: list, learning_rates: list, epochs: int, batch_size: int, X_train, y_train, X_val, y_val):\n",
        "  '''\n",
        "    Finds the best parameters for a given model.\n",
        "    architecture: int - the architecture of the model\n",
        "    optimizers: list - the list of optimizers to use\n",
        "    learning_rates: list - the list of learning rates to use\n",
        "    epochs: int - the number of epochs to run\n",
        "    batch_size: int - the batch size\n",
        "    X_train: torch.Tensor - the training features\n",
        "    y_train: torch.Tensor - the training labels\n",
        "    X_val: torch.Tensor - the validation features\n",
        "    y_val: torch.Tensor - the validation labels\n",
        "    return: dict - dictionary that contains the best parameters\n",
        "  '''\n",
        "  best_loss = float('inf')\n",
        "  best_params = None\n",
        "\n",
        "  for opt in optimizers:\n",
        "    for lr in learning_rates:\n",
        "      for bs in batch_size:\n",
        "        #print(f\"Running with optimizer: {opt}, learning rate: {lr}, batch size: {bs}\")\n",
        "        model = build_nn(architecture)\n",
        "        train_loader = DataLoader(list(zip(X_train, y_train)), batch_size=bs, shuffle=True)\n",
        "        val_loader = DataLoader(list(zip(X_val, y_val)), batch_size=bs, shuffle=False)\n",
        "\n",
        "        if opt == \"SGD\":\n",
        "          optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "        elif opt == \"Adam\":\n",
        "          optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "        elif opt == \"RMSprop\":\n",
        "          optimizer = optim.RMSprop(model.parameters(), lr=lr)\n",
        "        else:\n",
        "          optimizer = optim.Adagrad(model.parameters(), lr=lr)\n",
        "\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "          train_loss = train_loop(model, train_loader, loss_fn, optimizer)\n",
        "          val_loss = test_loop(model, val_loader, loss_fn)\n",
        "\n",
        "          train_accuracy = calculate_accuracy(model, train_loader)\n",
        "          val_accuracy = calculate_accuracy(model, val_loader)\n",
        "\n",
        "          writer.add_scalar(f\"Loss/train_{opt}_{lr}_{bs}\", train_loss, epoch)\n",
        "          writer.add_scalar(f\"Loss/test_{opt}_{lr}_{bs}\", val_loss, epoch)\n",
        "          writer.add_scalar(f\"train_accuracy_{opt}_{lr}_{bs}\", train_accuracy, epoch)\n",
        "          writer.add_scalar(f\"test_accuracy_{opt}_{lr}_{bs}\", val_accuracy, epoch)\n",
        "\n",
        "          if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            best_params = {\"lr\": lr, \"epoch\": epoch, \"bs\": bs, \"optimizer\": opt, \"accuracy\": val_accuracy}\n",
        "  return best_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "d43e450a",
      "metadata": {
        "id": "d43e450a"
      },
      "outputs": [],
      "source": [
        "learning_rates = [0.001, 0.01, 0.1]\n",
        "batch_sizes = [16, 32, 64]\n",
        "epochs = 1000\n",
        "optimizers = [\"SGD\", \"Adam\", \"RMSprop\", \"Adagrad\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "f0c421fc",
      "metadata": {
        "id": "f0c421fc"
      },
      "outputs": [],
      "source": [
        "# best parameters for each model\n",
        "model_1_params = parameter_tuning(1, optimizers, learning_rates, epochs, batch_sizes, X_train, y_train, X_val, y_val)\n",
        "model_2_params = parameter_tuning(2, optimizers, learning_rates, epochs, batch_sizes, X_train, y_train, X_val, y_val)\n",
        "model_3_params = parameter_tuning(3, optimizers, learning_rates, epochs, batch_sizes, X_train, y_train, X_val, y_val)\n",
        "model_4_params = parameter_tuning(4, optimizers, learning_rates, epochs, batch_sizes, X_train, y_train, X_val, y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "6e169c8f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e169c8f",
        "outputId": "394d7b39-c000-48ca-def9-6ca076f8c7b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model 1 parameters: {'lr': 0.1, 'epoch': 258, 'bs': 64, 'optimizer': 'SGD', 'accuracy': 0.8571428571428571}\n",
            "Model 2 parameters: {'lr': 0.1, 'epoch': 628, 'bs': 32, 'optimizer': 'SGD', 'accuracy': 0.8095238095238095}\n",
            "Model 3 parameters: {'lr': 0.01, 'epoch': 264, 'bs': 32, 'optimizer': 'RMSprop', 'accuracy': 0.9047619047619048}\n",
            "Model 4 parameters: {'lr': 0.1, 'epoch': 924, 'bs': 64, 'optimizer': 'SGD', 'accuracy': 0.8095238095238095}\n"
          ]
        }
      ],
      "source": [
        "print(f\"Model 1 parameters: {model_1_params}\")\n",
        "print(f\"Model 2 parameters: {model_2_params}\")\n",
        "print(f\"Model 3 parameters: {model_3_params}\")\n",
        "print(f\"Model 4 parameters: {model_4_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "425abfa8",
      "metadata": {
        "id": "425abfa8"
      },
      "outputs": [],
      "source": [
        "def test_model(architecture: int, best_params: dict, X_train, y_train, X_test, y_test):\n",
        "  '''\n",
        "    Evaluate the model with the best parameters using the test data\n",
        "    architecture: int - the architecture of the model\n",
        "    best_params: dict - the best parameters for the model\n",
        "    X_train: torch.Tensor - the training features\n",
        "    y_train: torch.Tensor - the training labels\n",
        "    X_test: torch.Tensor - the test features\n",
        "    y_test: torch.Tensor - the test labels\n",
        "    return: nn.Module - the trained model\n",
        "  '''\n",
        "  model = build_nn(architecture)\n",
        "  train_loader = DataLoader(list(zip(X_train, y_train)), batch_size=best_params[\"bs\"], shuffle=True)\n",
        "  test_loader = DataLoader(list(zip(X_test, y_test)), batch_size=best_params[\"bs\"], shuffle=False)\n",
        "\n",
        "  if best_params[\"optimizer\"] == \"SGD\":\n",
        "    optimizer = optim.SGD(model.parameters(), lr=best_params[\"lr\"])\n",
        "  elif best_params[\"optimizer\"] == \"Adam\":\n",
        "    optimizer = optim.Adam(model.parameters(), lr=best_params[\"lr\"])\n",
        "  elif best_params[\"optimizer\"] == \"RMSprop\":\n",
        "    optimizer = optim.RMSprop(model.parameters(), lr=best_params[\"lr\"])\n",
        "  else:\n",
        "    optimizer = optim.Adagrad(model.parameters(), lr=best_params[\"lr\"])\n",
        "\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "  for epoch in range(best_params[\"epoch\"]):\n",
        "    train_loss = train_loop(model, train_loader, loss_fn, optimizer)\n",
        "    writer.add_scalar(f\"Loss/train_{architecture}\", train_loss, epoch)\n",
        "  test_loss = test_loop(model, test_loader, loss_fn)\n",
        "  test_accuracy = calculate_accuracy(model, test_loader)\n",
        "  writer.add_scalar(f\"Loss/test_{architecture}\", test_loss, 0)\n",
        "  writer.add_scalar(f\"test_accuracy_{architecture}\", test_accuracy, 0)\n",
        "  print(f\"Test loss for model {architecture}: {test_loss}, Test accuracy for model {architecture}: {test_accuracy}\")\n",
        "\n",
        "  return model, test_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "c7ee094f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7ee094f",
        "outputId": "a4720881-d1ac-4f26-df4a-111841f39775"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss for model 1: 0.9682804942131042, Test accuracy for model 1: 0.6904761904761905\n",
            "Test loss for model 2: 0.8234452307224274, Test accuracy for model 2: 0.9047619047619048\n",
            "Test loss for model 3: 0.9325122535228729, Test accuracy for model 3: 0.7857142857142857\n",
            "Test loss for model 4: 0.46684715151786804, Test accuracy for model 4: 0.8095238095238095\n"
          ]
        }
      ],
      "source": [
        "model_1, model_1_accuracy = test_model(1, model_1_params, X_train, y_train, X_test, y_test)\n",
        "model_2, model_2_accuracy = test_model(2, model_2_params, X_train, y_train, X_test, y_test)\n",
        "model_3, model_3_accuracy = test_model(3, model_3_params, X_train, y_train, X_test, y_test)\n",
        "model_4, model_4_accuracy = test_model(4, model_4_params, X_train, y_train, X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracies = [model_1_accuracy, model_2_accuracy, model_3_accuracy, model_4_accuracy]\n",
        "best_model = np.argmax(accuracies) + 1"
      ],
      "metadata": {
        "id": "fhQm779uyh-a"
      },
      "id": "fhQm779uyh-a",
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(best_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEF3vqib9agK",
        "outputId": "6a6bc2c6-6912-41d2-b684-2e1407d6bc5b"
      },
      "id": "FEF3vqib9agK",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %load_ext tensorboard\n",
        "# %tensorboard --logdir logs"
      ],
      "metadata": {
        "id": "j2xPF134yw1c"
      },
      "id": "j2xPF134yw1c",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "bdc3199a-80a9-4a63-a2d7-56ed2d16755c",
      "metadata": {
        "id": "bdc3199a-80a9-4a63-a2d7-56ed2d16755c"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df1eaa39-d9bd-410c-beb5-9ca37bdf3d32",
      "metadata": {
        "id": "df1eaa39-d9bd-410c-beb5-9ca37bdf3d32"
      },
      "source": [
        "### Write your report here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "142f066b-5655-4e1e-9bb1-ae4cc5f54565",
      "metadata": {
        "id": "142f066b-5655-4e1e-9bb1-ae4cc5f54565"
      },
      "source": [
        "After loading the sonar dataset, I found it contained signal data used to distinguish between metal cylinders and rocks. Each sample had 60 features representing different sonar frequencies, with a binary label indicating either metal (M) or rock (R).  \n",
        "\n",
        "\n",
        "I implemented four different neural network architectures to explore various model complexities and activation functions. Model 1 used a simple 3-layer architecture with ReLU activation, Model 2 expanded to 5 layers with Tanh activation, Model 3 implemented a 7-layer network with Sigmoid activation and dropout layers for regularization, and Model 4 used 6 layers with Tanh activation. I chose these architectures to compare the impact of model complexity and activation functions on the model's performance.  \n",
        "\n",
        "For hyperparameter tuning, I experimented with multiple optimization algorithms (SGD, Adam, RMSprop, and Adagrad), learning rates [0.001, 0.01, 0.1], and batch sizes [16, 32, 64]. I split the data into training (70%), validation (20%), and test (10%) sets to properly evaluate model performance. I used the validation set for hyperparameter tuning to avoid contaminating the test set, which was reserved for final model evaluation.   \n",
        "\n",
        "After training and evaluating the models, I found that Model 2 achieved the highest test accuracy of 90.48% with learning rate 0.1, epoch 628, batch size 32, and SGD optimizer. What I learned from this is that sometimes simpler models can perform better than more complex models.\n",
        "In the future, I'll remember that while techniques like dropout and very deep architectures can be very useful, they are not always necessary and can sometimes hurt performance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6eeda53-ea91-4571-a35c-5dfbb3f6f32c",
      "metadata": {
        "id": "e6eeda53-ea91-4571-a35c-5dfbb3f6f32c"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "4dfd1c26-217d-400f-abf6-4b3a92e9a175",
      "metadata": {
        "id": "4dfd1c26-217d-400f-abf6-4b3a92e9a175"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "e353c9e6-e532-42c3-9288-3c50ed48a32f",
      "metadata": {
        "id": "e353c9e6-e532-42c3-9288-3c50ed48a32f"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}